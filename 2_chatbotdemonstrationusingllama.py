# -*- coding: utf-8 -*-
"""2_ChatbotDemonstrationusingLlama.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KpOFwVfixQDvR3s9-Dm5r95LLUucj6sv
"""

!pip install -U bitsandbytes

from huggingface_hub import login
#My token
login(token="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")
import pandas as pd
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
import time
import re
import matplotlib.pyplot as plt
import io
import base64
from IPython.display import display, HTML, Image

#Loaded Data from Excel Sheets
file_path = 'stocks_historical.xlsx'
sheet_names = ['AAPL', 'MSFT', 'NVDA', 'TSLA']
column_names = ['Date', 'Close', 'High', 'Low', 'Open', 'Volume']

df_list = []
try:
    for sheet in sheet_names:
        temp_df = pd.read_excel(
            file_path, sheet_name=sheet, skiprows=2,
            header=None, names=column_names
        )
        temp_df['Ticker'] = sheet
        df_list.append(temp_df)

    df = pd.concat(df_list, ignore_index=True)
    df['Date'] = pd.to_datetime(df['Date'])
    df.dropna(how='all', inplace=True)
    df.sort_values(by=['Ticker', 'Date'], inplace=True)
    print("All 4 sheets loaded and combined successfully.")
    display(df.head())
except Exception as e:
    print(f"An error occurred while loading the data: {e}")
    df = pd.DataFrame()


# ## Model Handler Layer
class ModelHandler:
    """ Manages loading and running different open-source LLMs with 4-bit quantization. """
    def __init__(self):
        self.pipelines = {}
    def load_model(self, model_id):
        if model_id not in self.pipelines:
            print(f"Loading quantized model: {model_id}...")
            tokenizer = AutoTokenizer.from_pretrained(model_id)
            model = AutoModelForCausalLM.from_pretrained(
                model_id,
                device_map="auto",
                load_in_4bit=True #Quantisized the model
            )

            self.pipelines[model_id] = pipeline(
                "text-generation",
                model=model,
                tokenizer=tokenizer,
                device_map="auto"
            )
            print(f"Quantized model {model_id} loaded successfully.")

    def generate(self, model_id, prompt):
        if model_id not in self.pipelines: self.load_model(model_id)
        pipe = self.pipelines[model_id]
        messages = [{"role": "user", "content": prompt}]
        prompt_formatted = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)

        outputs = pipe(
            prompt_formatted,
            max_new_tokens=256,
            do_sample=True,
            temperature=0.1,
            top_p=0.95
        )
        return outputs[0]['generated_text']

#Orchestration Layer
class Chatbot:
    """Orchestrates the process of converting natural language to data analysis results. """
    def __init__(self, dataframe, model_handler):
        self.df = dataframe
        self.handler = model_handler
        with io.StringIO() as buf:
            self.df.info(buf=buf)
            self.schema = buf.getvalue()

    def create_prompt(self, query):
        prompt_lines = [
            "You are a helpful and expert Python data analyst.", "Your task is to write a Python script to answer a user's question about a pandas DataFrame.",
            "The DataFrame, named `df`, is already loaded in memory.", f"Here is the schema of the DataFrame:\n{self.schema}",
            "The user's question is:", f'"{query}"',
            "Please write the Python code to answer the question. The final result must be stored in a variable named `result`.",
            "Your response must contain ONLY the Python code, enclosed in a markdown block."
        ]
        return "\n".join(prompt_lines)

    def extract_code(self, response):
        answer_start_token = "<|start_header_id|>assistant\n"
        answer_start_index = response.find(answer_start_token)
        model_answer = response[answer_start_index + len(answer_start_token):] if answer_start_index != -1 else response

        match = re.search(r'```(?:python)?\s*\n(.*?)```', model_answer, re.DOTALL)
        if match:
            return match.group(1).strip()
        return None

    def execute_code(self, code):
        if not code: return None, "Error: No code generated."
        local_scope = {'df': self.df, 'pd': pd, 'plt': plt, 'io': io}
        try:
            exec(code, {}, local_scope)
            return local_scope.get('result', None), None
        except Exception as e:
            return None, f"Execution Error: {e}"

    def ask(self, model_id, query):
        print(f"--- Querying {model_id} ---\n")
        prompt = self.create_prompt(query)
        start_time = time.time()
        raw_response = self.handler.generate(model_id, prompt)
        latency = time.time() - start_time

        #Code Extraction
        code = self.extract_code(raw_response)

        print(f"Query: '{query}'")
        print(f"Latency: {latency:.2f} seconds")

        print("\nGenerated Code:")
        print(f"```python\n{code}\n```" if code else "No code snippet found.")

        result, error = self.execute_code(code)
        print("\nExecution Result:")
        if error: print(error)
        elif isinstance(result, io.BytesIO):
            img_data = base64.b6encode(result.read()).decode('utf-8')
            display(HTML(f'<img src="data:image/png;base64,{img_data}">'))
        elif result is not None: display(result)
        else: print("Code executed, but no 'result' variable was found.")
        print("\n" + "="*50 + "\n")


#Labelling the models used
if not df.empty:
    QWEN_ID = "Qwen/Qwen1.5-0.5B-Chat"
    TINYLAMA_ID = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
    LLAMA3_ID = "meta-llama/Meta-Llama-3-8B-Instruct"
    GEMMA_ID = "google/gemma-1.1-7b-it"

    # Creating objects
    model_handler = ModelHandler()
    chatbot = Chatbot(df, model_handler)

    # Asking the questions
    chatbot.ask(LLAMA3_ID, "What was the closing price for MSFT on 2023-10-26?")
    chatbot.ask(LLAMA3_ID, "What was the average trading volume for NVDA in 2022?")
    chatbot.ask(LLAMA3_ID, "Retrieve the closing prices for both AAPL and TSLA on 2022-11-11.")
    chatbot.ask(LLAMA3_ID, "Plot the closing prices of MSFT and NVDA throughout 2023 on the same chart.")
else:
    print("DataFrame could not be loaded.")