# -*- coding: utf-8 -*-
"""3_AutomatedEvaluation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1m-ZSZm9MHeEZL4eNbFaakKyiKRWIPgld
"""

# # Notebook 3: Automated Benchmark Evaluation

#Setup and Imports
from huggingface_hub import login

# My token
login(token="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")

import pandas as pd
import numpy as np
from tqdm.notebook import tqdm
import warnings
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
import time
import re
import io

warnings.filterwarnings('ignore')


# Model Handler and Orchestration Layer
class ModelHandler:
    def __init__(self):
        self.pipelines = {}
        self.torch_dtype = torch.bfloat16 if torch.cuda.is_available() and torch.cuda.get_device_capability()[0] >= 8 else torch.float32

    def load_model(self, model_id):
        if model_id not in self.pipelines:
            print(f"Loading model: {model_id}...")
            tokenizer = AutoTokenizer.from_pretrained(model_id)
            model = AutoModelForCausalLM.from_pretrained(
                model_id, torch_dtype=self.torch_dtype, device_map="auto"
            )
            self.pipelines[model_id] = pipeline(
                "text-generation", model=model, tokenizer=tokenizer,
                torch_dtype=self.torch_dtype, device_map="auto"
            )
            print(f"Model {model_id} loaded successfully.")

    def generate(self, model_id, prompt):
        if model_id not in self.pipelines: self.load_model(model_id)
        pipe = self.pipelines[model_id]
        messages = [{"role": "user", "content": prompt}]
        prompt_formatted = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
        outputs = pipe(
            prompt_formatted, max_new_tokens=256, do_sample=False,
            temperature=0.0, top_p=1.0, pad_token_id=pipe.tokenizer.eos_token_id
        )
        return outputs[0]['generated_text']

class Chatbot:
    def __init__(self, dataframe, model_handler):
        self.df = dataframe
        self.handler = model_handler
        with io.StringIO() as buf:
            self.df.info(buf=buf)
            self.schema = buf.getvalue()

    def create_prompt(self, query):
        # Using the final, improved prompt
        prompt_lines = [
            "You are an obedient and expert Python data analyst. Your ONLY task is to write a short snippet of Python code to answer a question.", "",
            "== RULES ==",
            "1. **YOU MUST** use the pandas DataFrame already in memory named `df`.",
            "2. **DO NOT** write code to read any file (ABSOLUTELY NO `pd.read_csv` or `pd.read_excel`).",
            "3. Your code snippet **MUST** end by assigning the final answer to a variable called `result`.",
            "4. Only output the Python code, enclosed in triple backticks.", "",
            "== DATA SCHEMA ==", "The DataFrame `df` has the following columns and data types:",
            "```", self.schema, "```", "",
            "== EXAMPLE ==", 'Question: "What was the highest closing price for AAPL?"',
            "```python", "result = df[df['Ticker'] == 'AAPL']['Close'].max()", "```", "",
            "== YOUR TASK ==", f'Question: "{query}"'
        ]
        return "\n".join(prompt_lines)

    def extract_code(self, response):
        match = re.search(r'```python\n(.*?)\n```', response, re.DOTALL)
        return match.group(1).strip() if match else None

    def execute_code(self, code):
        if not code: return None, "Error: No code generated."
        local_scope = {'df': self.df, 'pd': pd}
        try:
            exec(code, {}, local_scope)
            return local_scope.get('result', None), None
        except Exception as e:
            return None, f"Execution Error: {e}"


# ## 3. Load Data and Benchmark Questions
# Load the full dataset first
file_path = 'stocks_historical.xlsx'
sheet_names = ['AAPL', 'MSFT', 'NVDA', 'TSLA']
column_names = ['Date', 'Close', 'High', 'Low', 'Open', 'Volume']
df_list = []
for sheet in sheet_names:
    temp_df = pd.read_excel(file_path, sheet_name=sheet, skiprows=2, header=None, names=column_names)
    temp_df['Ticker'] = sheet
    df_list.append(temp_df)
df = pd.concat(df_list, ignore_index=True)
df['Date'] = pd.to_datetime(df['Date'])

df_benchmark = df[df['Ticker'] == 'AAPL'].copy()
print("Benchmark will run on AAPL data only for consistency.")

# For this demo, we use a sample of 5 questions from our evaluation_questions.csv.
# For our final analysis, we  replace this with the full 100 questions.
questions_data = {
    'ID': [1, 26, 51, 76, 54],
    'Category': ['Simple Retrieval', 'Basic Aggregation', 'Conditional Analysis', 'Complex Calculation', 'Conditional Analysis'],
    'Question': [
        "What was the closing price on 2023-10-26?",
        "What was the average closing price in 2021?",
        "Retrieve all rows where High > 150.",
        "Calculate a 7-day moving average of the Close price.",
        "Count how many days had Close > Open."
    ]
}
questions_df = pd.DataFrame(questions_data)

# 4. Pre-calculating Ground Truth Answers
def get_ground_truth(q_id, dataframe):
    df_copy = dataframe.copy()
    try:
        if q_id == 1: return df_copy[df_copy['Date'] == '2023-10-26']['Close'].iloc[0]
        elif q_id == 26: return df_copy[df_copy['Date'].dt.year == 2021]['Close'].mean()
        elif q_id == 51: return df_copy[df_copy['High'] > 150]
        elif q_id == 76: return df_copy['Close'].rolling(window=7).mean()
        elif q_id == 54: return (df_copy['Close'] > df_copy['Open']).sum()
        else: return None
    except Exception: return None

questions_df['ground_truth'] = questions_df['ID'].apply(lambda q_id: get_ground_truth(q_id, df_benchmark))
print("Ground truth calculation complete.")

# 5. Running Automated Evaluation
QWEN_ID = "Qwen/Qwen1.5-0.5B-Chat"
TINYLAMA_ID = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
GEMMA_ID = "google/gemma-1.1-7b-it"
LLAMA_ID = "meta-llama/Llama-2-7b-chat-hf"

models_to_test = [QWEN_ID,TINYLAMA_ID]

model_handler = ModelHandler()
chatbot = Chatbot(df_benchmark, model_handler)

results = []
for model_id in models_to_test:
    for _, row in tqdm(questions_df.iterrows(), total=len(questions_df), desc=f"Testing {model_id}"):
        prompt = chatbot.create_prompt(row['Question'])
        start_time = time.time()
        raw_response = chatbot.handler.generate(model_id, prompt)
        latency = time.time() - start_time
        code = chatbot.extract_code(raw_response)
        result, error = chatbot.execute_code(code)
        passed = 0
        ground_truth = row['ground_truth']
        if error is None and result is not None:
            try:
                if isinstance(result, pd.DataFrame): passed = 1 if result.equals(ground_truth) else 0
                elif isinstance(result, pd.Series): passed = 1 if result.equals(ground_truth) else 0
                elif isinstance(result, (int, float)): passed = 1 if np.isclose(result, ground_truth) else 0
                else: passed = 1 if result == ground_truth else 0
            except: passed = 0

        results.append({
            'model_id': model_id, 'question_id': row['ID'], 'latency': latency,
            'error': error, 'passed': passed
        })
# ## 6. Saving Results to results.csv
results_df = pd.DataFrame(results)
results_df.to_csv('results.csv', index=False)
print("\nBenchmark complete! Results saved to 'results.csv'.")
display(results_df)



!pip install -U bitsandbytes

# This version uses 4-bit quantization to run the benchmark much faster.
from huggingface_hub import login
login(token="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")

# 1. Setup and Imports
import pandas as pd
import numpy as np
from tqdm.notebook import tqdm
import warnings
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
import time
import re
import io

warnings.filterwarnings('ignore')

# 2.Helper Classes (with Quantization)
class ModelHandler:
    """ Managing loading and running quantized open-source LLMs. """
    def __init__(self):
        self.pipelines = {}

    def load_model(self, model_id):
        if model_id not in self.pipelines:
            print(f"Loading quantized model: {model_id}...")
            tokenizer = AutoTokenizer.from_pretrained(model_id)

            # key change: load the model in 4-bit
            model = AutoModelForCausalLM.from_pretrained(
                model_id,
                device_map="auto",
                load_in_4bit=True
            )

            self.pipelines[model_id] = pipeline(
                "text-generation",
                model=model,
                tokenizer=tokenizer,
                device_map="auto"
            )
            print(f"Quantized model {model_id} loaded successfully.")

    def generate(self, model_id, prompt):
        if model_id not in self.pipelines: self.load_model(model_id)
        pipe = self.pipelines[model_id]
        messages = [{"role": "user", "content": prompt}]
        prompt_formatted = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)

        outputs = pipe(
            prompt_formatted,
            max_new_tokens=256,
            do_sample=True,
            temperature=0.1,
            top_p=0.95
        )
        return outputs[0]['generated_text']

class Chatbot:

    def __init__(self, dataframe, model_handler):
        self.df = dataframe
        self.handler = model_handler
        with io.StringIO() as buf:
            self.df.info(buf=buf)
            self.schema = buf.getvalue()

    def create_prompt(self, query):
        # Using the final, successful prompt for capable models
        prompt_lines = [
            "You are a helpful and expert Python data analyst.",
            "Your task is to write a Python script to answer a user's question about a pandas DataFrame.",
            "The DataFrame, named `df`, is already loaded in memory.",
            f"Here is the schema of the DataFrame:\n{self.schema}",
            "The user's question is:",
            f'"{query}"',
            "Please write the Python code to answer the question. The final result must be stored in a variable named `result`.",
            "Your response must contain ONLY the Python code, enclosed in a markdown block."
        ]
        return "\n".join(prompt_lines)

    def extract_code(self, response):
        # This part extracts just the model's answer from the full response text
        answer_start_token = "<|start_header_id|>assistant\n"
        answer_start_index = response.find(answer_start_token)
        model_answer = response[answer_start_index + len(answer_start_token):] if answer_start_index != -1 else response

        match = re.search(r'```python\n(.*?)\n```', model_answer, re.DOTALL)
        return match.group(1).strip() if match else None

    def execute_code(self, code):
        if not code: return None, "Error: No code generated."
        local_scope = {'df': self.df, 'pd': pd}
        try:
            exec(code, {}, local_scope)
            return local_scope.get('result', None), None
        except Exception as e:
            return None, f"Execution Error: {e}"

# 3. Load Data and Benchmark Questions
file_path = 'stocks_historical.xlsx'
sheet_names = ['AAPL', 'MSFT', 'NVDA', 'TSLA']
column_names = ['Date', 'Close', 'High', 'Low', 'Open', 'Volume']
df_list = []
for sheet in sheet_names:
    temp_df = pd.read_excel(file_path, sheet_name=sheet, skiprows=2, header=None, names=column_names)
    temp_df['Ticker'] = sheet
    df_list.append(temp_df)
df = pd.concat(df_list, ignore_index=True)
df['Date'] = pd.to_datetime(df['Date'])

#single stock to match the dissertation's benchmark scope
df_benchmark = df[df['Ticker'] == 'AAPL'].copy()
print("Benchmark will run on AAPL data only for consistency.")

# Sample of 5 questions from your dissertation's Appendix A.
questions_data = {
    'ID': [1, 26, 51, 76, 54],
    'Category': ['Simple Retrieval', 'Basic Aggregation', 'Conditional Analysis', 'Complex Calculation', 'Conditional Analysis'],
    'Question': [
        "What was the closing price on 2023-10-26?",
        "What was the average closing price in 2021?",
        "Retrieve all rows where High > 150.",
        "Calculate a 7-day moving average of the Close price.",
        "Count how many days had Close > Open."
    ]
}
questions_df = pd.DataFrame(questions_data)


# 4. Pre-calculating Ground Truth Answers
def get_ground_truth(q_id, dataframe):
    df_copy = dataframe.copy()
    try:
        if q_id == 1: return df_copy[df_copy['Date'] == '2023-10-26']['Close'].iloc[0]
        elif q_id == 26: return df_copy[df_copy['Date'].dt.year == 2021]['Close'].mean()
        elif q_id == 51: return df_copy[df_copy['High'] > 150]
        elif q_id == 76: return df_copy['Close'].rolling(window=7).mean()
        elif q_id == 54: return (df_copy['Close'] > df_copy['Open']).sum()
        else: return None
    except Exception: return None

questions_df['ground_truth'] = questions_df['ID'].apply(lambda q_id: get_ground_truth(q_id, df_benchmark))
print("Ground truth calculation complete.")

# 5. Running Automated Evaluation
models_to_test = ["google/gemma-1.1-7b-it"]

model_handler = ModelHandler()
chatbot = Chatbot(df_benchmark, model_handler)

results = []
for model_id in models_to_test:
    for _, row in tqdm(questions_df.iterrows(), total=len(questions_df), desc=f"Testing {model_id}"):
        prompt = chatbot.create_prompt(row['Question'])
        start_time = time.time()
        raw_response = chatbot.handler.generate(model_id, prompt)
        latency = time.time() - start_time
        code = chatbot.extract_code(raw_response)
        result, error = chatbot.execute_code(code)

        passed = 0
        ground_truth = row['ground_truth']
        if error is None and result is not None:
            try:
                if isinstance(result, pd.DataFrame): passed = 1 if result.equals(ground_truth) else 0
                elif isinstance(result, pd.Series): passed = 1 if result.equals(ground_truth) else 0
                elif isinstance(result, (int, float)): passed = 1 if np.isclose(result, ground_truth) else 0
                else: passed = 1 if result == ground_truth else 0
            except: passed = 0

        results.append({
            'model_id': model_id, 'question_id': row['ID'], 'latency': latency,
            'error': str(error) if error else None, 'passed': passed
        })

# ## 6. Save Results
results_df = pd.DataFrame(results)
results_df.to_csv('results.csv', index=False)
print("\nBenchmark complete! Results saved to 'results.csv'.")
display(results_df)

from huggingface_hub import login
login(token="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")

# 1. Setup and Imports
import pandas as pd
import numpy as np
from tqdm.notebook import tqdm
import warnings
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
import time
import re
import io

warnings.filterwarnings('ignore')

class ModelHandler:
    """ Manages loading and running quantized open-source LLMs. """
    def __init__(self):
        self.pipelines = {}

    def load_model(self, model_id):
        if model_id not in self.pipelines:
            print(f"Loading quantized model: {model_id}...")
            tokenizer = AutoTokenizer.from_pretrained(model_id)
            model = AutoModelForCausalLM.from_pretrained(
                model_id,
                device_map="auto",
                load_in_4bit=True
            )
            self.pipelines[model_id] = pipeline(
                "text-generation",
                model=model,
                tokenizer=tokenizer,
                device_map="auto"
            )
            print(f"Quantized model {model_id} loaded successfully.")

    def generate(self, model_id, prompt):
        if model_id not in self.pipelines: self.load_model(model_id)
        pipe = self.pipelines[model_id]
        messages = [{"role": "user", "content": prompt}]


        terminators = [
            pipe.tokenizer.eos_token_id,
            pipe.tokenizer.convert_tokens_to_ids("<|eot_id|>")
        ]

        prompt_formatted = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
        outputs = pipe(
            prompt_formatted,
            max_new_tokens=256,
            eos_token_id=terminators,
            do_sample=True,
            temperature=0.1,
            top_p=0.95
        )
        return outputs[0]['generated_text']

class Chatbot:
    """ Orchestrates the process of converting natural language to data analysis results. """
    def __init__(self, dataframe, model_handler):
        self.df = dataframe
        self.handler = model_handler
        with io.StringIO() as buf:
            self.df.info(buf=buf)
            self.schema = buf.getvalue()

    def create_prompt(self, query):
        prompt_lines = [
            "You are a helpful and expert Python data analyst.",
            "Your task is to write a Python script to answer a user's question about a pandas DataFrame.",
            "The DataFrame, named `df`, is already loaded in memory.",
            f"Here is the schema of the DataFrame:\n{self.schema}",
            "The user's question is:", f'"{query}"',
            "Please write the Python code to answer the question. The final result must be stored in a variable named `result`.",
            "Your response must contain ONLY the Python code, enclosed in a markdown block."
        ]
        return "\n".join(prompt_lines)

    def extract_code(self, response):

        assistant_token = "<|start_header_id|>assistant<|end_header_id|>"
        assistant_start_index = response.find(assistant_token)
        if assistant_start_index == -1:
            return None

        model_answer = response[assistant_start_index + len(assistant_token):]
        match = re.search(r'```(?:python)?\s*\n(.*?)```', model_answer, re.DOTALL)
        if match:
            return match.group(1).strip()
        return None

    def execute_code(self, code):
        if not code: return None, "Error: No code generated."
        local_scope = {'df': self.df, 'pd': pd}
        try:
            exec(code, {}, local_scope)
            return local_scope.get('result', None), None
        except Exception as e:
            return None, f"Execution Error: {e}"
#  Load Data and Benchmark Questions
file_path = 'stocks_historical.xlsx'
sheet_names = ['AAPL', 'MSFT', 'NVDA', 'TSLA']
column_names = ['Date', 'Close', 'High', 'Low', 'Open', 'Volume']
df_list = []
for sheet in sheet_names:
    temp_df = pd.read_excel(file_path, sheet_name=sheet, skiprows=2, header=None, names=column_names)
    temp_df['Ticker'] = sheet
    df_list.append(temp_df)
df = pd.concat(df_list, ignore_index=True)
df['Date'] = pd.to_datetime(df['Date'])

df_benchmark = df[df['Ticker'] == 'AAPL'].copy()
print("Benchmark will run on AAPL data only for consistency.")

questions_data = {
    'ID': [1, 26, 51, 76, 54], 'Category': ['Simple Retrieval', 'Basic Aggregation', 'Conditional Analysis', 'Complex Calculation', 'Conditional Analysis'],
    'Question': [
        "What was the closing price on 2023-10-26?", "What was the average closing price in 2021?",
        "Retrieve all rows where High > 150.", "Calculate a 7-day moving average of the Close price.",
        "Count how many days had Close > Open."
    ]
}
questions_df = pd.DataFrame(questions_data)

# Pre-calculate Ground Truth Answers
def get_ground_truth(q_id, dataframe):
    df_copy = dataframe.copy()
    try:
        if q_id == 1: return df_copy[df_copy['Date'] == '2023-10-26']['Close'].iloc[0]
        elif q_id == 26: return df_copy[df_copy['Date'].dt.year == 2021]['Close'].mean()
        elif q_id == 51: return df_copy[df_copy['High'] > 150]
        elif q_id == 76: return df_copy['Close'].rolling(window=7).mean()
        elif q_id == 54: return (df_copy['Close'] > df_copy['Open']).sum()
        else: return None
    except Exception: return None

questions_df['ground_truth'] = questions_df['ID'].apply(lambda q_id: get_ground_truth(q_id, df_benchmark))
print("Ground truth calculation complete.")

# Running Automated Evaluation on Llama 3
models_to_test = ["meta-llama/Meta-Llama-3-8B-Instruct"]

model_handler = ModelHandler()
chatbot = Chatbot(df_benchmark, model_handler)

results = []
for model_id in models_to_test:
    for _, row in tqdm(questions_df.iterrows(), total=len(questions_df), desc=f"Testing {model_id}"):
        prompt = chatbot.create_prompt(row['Question'])
        start_time = time.time()
        raw_response = chatbot.handler.generate(model_id, prompt)
        latency = time.time() - start_time
        code = chatbot.extract_code(raw_response)
        result, error = chatbot.execute_code(code)

        passed = 0
        ground_truth = row['ground_truth']
        if error is None and result is not None:
            try:
                if isinstance(result, pd.DataFrame): passed = 1 if result.equals(ground_truth) else 0
                elif isinstance(result, pd.Series): passed = 1 if result.equals(ground_truth) else 0
                elif isinstance(result, (int, float)): passed = 1 if np.isclose(result, ground_truth) else 0
                else: passed = 1 if result == ground_truth else 0
            except: passed = 0
        results.append({
            'model_id': model_id, 'question_id': row['ID'], 'latency': latency,
            'error': str(error) if error else None, 'passed': passed
        })

#Save Results
results_df = pd.DataFrame(results)
results_df.to_csv('results_llama3.csv', index=False)
print("\nBenchmark for Llama 3 complete! Results saved to 'results_llama3.csv'.")
display(results_df)